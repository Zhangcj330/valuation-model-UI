{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote, urlparse\n",
    "\n",
    "\n",
    "def normalize_url( url: str) -> str:\n",
    "    \"\"\"Normalize the SharePoint URL to ensure compatibility\"\"\"\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    path = unquote(parsed_url.path).strip('/')\n",
    "    # Construct the Graph API path\n",
    "    # Assuming the path is something like '/sites/SiteName/Shared Documents/...'\n",
    "    if path.startswith('sites/'):\n",
    "        path_parts = path.split('/', 3)\n",
    "        if len(path_parts) > 3:\n",
    "            site_path = path_parts[3]  # This should be the path after '/sites/SiteName/'\n",
    "        else:\n",
    "            site_path = ''\n",
    "    else:\n",
    "        site_path = path\n",
    "\n",
    "    return site_path\n",
    "\n",
    "\n",
    "url = \"https://datalyactuarial.sharepoint.com/sites/DatalyActuarial/Shared%20Documents/Business%20Development%20and%20Thought%20Leadership/Valuation%20Model%20Development/Run1/models\"\n",
    "normalize_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import get_model_handler\n",
    "import pandas as pd\n",
    "from sharepoint_utils import SharePointClient  # Import directly\n",
    "\n",
    "# Your token from browser\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> fcaa870 (validation check for IP)
    "# Create SharePoint client directly (bypassing get_model_handler)\n",
    "sp_client = SharePointClient(token=token)\n",
    "\n",
    "# Test URL\n",
    "test_url = \"https://datalyactuarial.sharepoint.com/sites/DatalyActuarial/Shared%20Documents/Business%20Development%20and%20Thought%20Leadership/Valuation%20Model%20Development/Python%20Models/IP%20Model%20Resources/IP_Model_Source_Data\"\n",
    "\n",
    "def download_assumptions_IP(url):\n",
    "# download all files in the folder\n",
    "    files = sp_client.list_files(url)\n",
    "    assumptions_dict = {}\n",
    "    for file in files:\n",
    "        if file.endswith(\".xlsx\") or file.endswith(\".xls\"):\n",
    "            assumption_file = sp_client.download_file(f\"{url}/{file}\")\n",
    "            # Get all sheet names\n",
    "            excel_file = pd.ExcelFile(assumption_file)\n",
    "            \n",
    "            # Read each sheet into the dictionary\n",
    "            for sheet_name in excel_file.sheet_names:\n",
    "                df = pd.read_excel(assumption_file, sheet_name=sheet_name)\n",
    "                assumptions_dict[sheet_name] = df\n",
    "    return assumptions_dict\n",
    "\n",
    "# Try a simple request\n",
    "try:\n",
    "    assumptions_dict = download_assumptions_IP(test_url)\n",
    "    print(\"Success!\")\n",
    "    print(\"\\nAssumption tables loaded:\")\n",
    "    for sheet_name, df in assumptions_dict.items():\n",
    "        print(f\"\\nSheet: {sheet_name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions_dict[\"Inflation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def transform_assumptions(assumptions_dict):\n",
    "    \"\"\"\n",
    "    Transform all assumption tables and return them in a dictionary\n",
    "\n",
    "    Args:\n",
    "        assumptions_dict: Dictionary of raw assumption DataFrames\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of transformed DataFrames\n",
    "    \"\"\"\n",
    "    transformed = {}\n",
    "\n",
    "    mapping_tables = [\n",
    "        \"Occupation\",\n",
    "        \"Waiting_period\",\n",
    "        \"Smoker\",\n",
    "        \"Benefit_period\",\n",
    "        \"Prem_payment_freq\",\n",
    "    ]\n",
    "\n",
    "    for table in mapping_tables:\n",
    "        if table in assumptions_dict:\n",
    "            transformed[table] = assumptions_dict[table].copy()\n",
    "\n",
    "    # 1. Simple direct assignments (no transformations needed)\n",
    "    simple_tables = [\n",
    "        \"Mortality\",\n",
    "        \"Lapse\",\n",
    "        \"TPD\",\n",
    "        \"Trauma\",\n",
    "        \"Monthly_discount_rates\",\n",
    "        \"Commission_rates\",\n",
    "        \"Prem_related_expenses\",\n",
    "        \"Fixed_expenses\",\n",
    "        \"Risk_adj_pc\",\n",
    "        \"Variables\",\n",
    "        \"Termination_new_claim\",\n",
    "        \"Termination_cause_of_sickness\",\n",
    "    ]\n",
    "\n",
    "    for table in simple_tables:\n",
    "        if table in assumptions_dict:\n",
    "            transformed[table] = assumptions_dict[table].copy()\n",
    "\n",
    "    # Premium rate tables with Y/N to S/N transformation\n",
    "    premium_tables = [\n",
    "        \"Prem_rate_level\",\n",
    "        \"Prem_rate_stepped\",\n",
    "        \"Rein_Prem_rate_level\",\n",
    "        \"Rein_Prem_rate_stepped\",\n",
    "    ]\n",
    "\n",
    "    for table in premium_tables:\n",
    "        if table in assumptions_dict:\n",
    "            df = assumptions_dict[table].copy()\n",
    "            # Only transform the 'Smoker status' column\n",
    "            df[\"Smoker status\"] = df[\"Smoker status\"].map({\"Y\": \"S\", \"N\": \"N\"})\n",
    "            transformed[table] = df\n",
    "\n",
    "    # 2. Death Only Mortality transformations\n",
    "    df_death_only_mort = assumptions_dict[\"DeathOnly_mort_age_rates\"].copy()\n",
    "    Death_Only_Mort_Age_Rates = df_death_only_mort.rename(\n",
    "        columns={\n",
    "            \"Sex\": \"sex\",\n",
    "            \"Age last birthday at last policy anniversary\": \"Age LB\",\n",
    "            \"Non-smoker\": \"N\",\n",
    "            \"Smoker\": \"S\",\n",
    "        }\n",
    "    )\n",
    "    Death_Only_Mort_Age_Rates = Death_Only_Mort_Age_Rates.drop(columns=\"Aggregate\")\n",
    "    transformed[\"Death_Only_Mort_Age_Rates\"] = pd.melt(\n",
    "        Death_Only_Mort_Age_Rates,\n",
    "        id_vars=[\"Age LB\", \"sex\"],\n",
    "        var_name=\"Smoker status\",\n",
    "        value_name=\"Mortality Age Rates\",\n",
    "    )\n",
    "\n",
    "    # 3. Death Only Duration Loading\n",
    "    df_death_only_duration = assumptions_dict[\"DeathOnly_duration_loading\"].copy()\n",
    "    transformed[\"Death_Only_Duration_Loading\"] = (\n",
    "        pd.melt(\n",
    "            df_death_only_duration,\n",
    "            id_vars=[\"Policy Duration (Curtate Years)\"],\n",
    "            var_name=\"sex\",\n",
    "            value_name=\"Duration Loading\",\n",
    "        )\n",
    "        .assign(\n",
    "            **{\n",
    "                \"Policy Duration (Curtate Years)\": lambda x: x[\n",
    "                    \"Policy Duration (Curtate Years)\"\n",
    "                ].astype(str)\n",
    "            }\n",
    "        )\n",
    "        .replace({\"sex\": {\"Male\": \"M\", \"Female\": \"F\"}})\n",
    "    )\n",
    "\n",
    "    # 4. Incidence Age Rates (Female)\n",
    "    df_incidence_female = assumptions_dict[\"Incidence_age_rates_females\"].copy()\n",
    "    transformed[\"Incidence_Age_Rates_Female\"] = df_incidence_female.rename(\n",
    "        columns={\n",
    "            \"Age\": \"Age LB\",\n",
    "            \"Accident\": \"Accident Age Rates\",\n",
    "            \"Sickness\": \"Sick Age Rates\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 5. Incidence Age Rates (Male)\n",
    "    df_incidence_male = assumptions_dict[\"Incidence_age_rates_males\"].copy()\n",
    "    male_rates = df_incidence_male.rename(columns={\"Age\": \"Age LB\"})\n",
    "    transformed[\"Incidence_Age_Rates_Male\"] = pd.melt(\n",
    "        male_rates,\n",
    "        id_vars=[\"Sex\", \"Age LB\"],\n",
    "        var_name=\"Accident Incidence Type\",\n",
    "        value_name=\"Accident Age Rates\",\n",
    "    ).replace(\n",
    "        {\n",
    "            \"Accident Incidence Type\": {\n",
    "                \"Accident Combined White Collar\": \"W\",\n",
    "                \"Accident Combined Blue Collar\": \"B\",\n",
    "                \"Sickness\": \"S\",\n",
    "            }\n",
    "        }\n",
    "    )[\n",
    "        [\"Age LB\", \"Sex\", \"Accident Incidence Type\", \"Accident Age Rates\"]\n",
    "    ]\n",
    "\n",
    "    # 6. Incidence Lifetime Benefit Period\n",
    "    df_lifetime_benefit = assumptions_dict[\"Incidence_lifetime_bene_period\"].copy()\n",
    "    transformed[\"Incidence_Lifetime_Benefit_Period\"] = df_lifetime_benefit.rename(\n",
    "        columns={\n",
    "            \"Accident\": \"Accident Lifetime Factor\",\n",
    "            \"Sickness\": \"Sick Lifetime Factor\",\n",
    "            \"Sex\": \"sex\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 7. Incidence Waiting Period\n",
    "    df_waiting_period = assumptions_dict[\"Incidence_waiting_period\"].copy()\n",
    "    occupation_mapping = {\n",
    "        \"Professional/Medical\": \"P\",\n",
    "        \"White Collar\": \"W\",\n",
    "        \"Sedentary\": \"S\",\n",
    "        \"Trades-person\": \"T\",\n",
    "        \"Blue/Heavy Blue Collar\": \"B\",\n",
    "    }\n",
    "    waiting_period = pd.melt(\n",
    "        df_waiting_period,\n",
    "        id_vars=[\"Type\", \"Sex\", \"Waiting_Period\"],\n",
    "        var_name=\"Occupation\",\n",
    "        value_name=\"Waiting Factor\",\n",
    "    )\n",
    "    waiting_period[\"Occupation\"] = waiting_period[\"Occupation\"].map(occupation_mapping)\n",
    "    transformed[\"Incidence_Waiting_Period\"] = (\n",
    "        waiting_period.pivot_table(\n",
    "            index=[\"Sex\", \"Waiting_Period\", \"Occupation\"],\n",
    "            columns=\"Type\",\n",
    "            values=\"Waiting Factor\",\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\"Accident\": \"Accident Wait Factor\", \"Sickness\": \"Sick Wait Factor\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 8. Incidence Smoking Status\n",
    "    df_smoking_status = assumptions_dict[\"Incidence_smoking_status\"].copy()\n",
    "    occupation_mapping = {\n",
    "        \"Combined White Collar\": [\"W\", \"P\"],\n",
    "        \"Combined Blue Collar\": [\"S\", \"T\", \"B\"],\n",
    "    }\n",
    "    smoking_mapping = {\"Smoker\": \"S\", \"Non-smoker\": \"N\"}\n",
    "\n",
    "    smoking_status = pd.melt(\n",
    "        df_smoking_status,\n",
    "        id_vars=[\"Type\", \"Sex\", \"Smoking_Status\"],\n",
    "        var_name=\"Occupation Type\",\n",
    "        value_name=\"Smoker Factor\",\n",
    "    )\n",
    "    smoking_status[\"Smoking_Status\"] = smoking_status[\"Smoking_Status\"].map(\n",
    "        smoking_mapping\n",
    "    )\n",
    "\n",
    "    expanded_rows = []\n",
    "    for _, row in smoking_status.iterrows():\n",
    "        occupation_codes = occupation_mapping[row[\"Occupation Type\"]]\n",
    "        for code in occupation_codes:\n",
    "            new_row = row.copy()\n",
    "            new_row[\"Occupation\"] = code\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    smoking_status_transformed = pd.DataFrame(expanded_rows)\n",
    "    smoking_status_transformed = smoking_status_transformed.drop(\n",
    "        columns=[\"Occupation Type\"]\n",
    "    )\n",
    "    transformed[\"Incidence_Smoking_Status\"] = (\n",
    "        smoking_status_transformed.pivot_table(\n",
    "            index=[\"Sex\", \"Smoking_Status\", \"Occupation\"],\n",
    "            columns=\"Type\",\n",
    "            values=\"Smoker Factor\",\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"Accident\": \"Accident Smoke Factor\",\n",
    "                \"Sickness\": \"Sick Smoke Factor\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 9. Incidence Benefit Type\n",
    "    df_benefit_type = assumptions_dict[\"Incidence_benefit_type\"].copy()\n",
    "    benefit_type_mapping = {\"Agreed Value\": \"A\", \"Indemnity\": \"I\"}\n",
    "\n",
    "    benefit_type = pd.melt(\n",
    "        df_benefit_type,\n",
    "        id_vars=[\"Type\", \"Sex\", \"Benefit Type\"],\n",
    "        var_name=\"Occupation Type\",\n",
    "        value_name=\"Benefit Type Factor\",\n",
    "    )\n",
    "    benefit_type[\"Benefit Type\"] = benefit_type[\"Benefit Type\"].map(\n",
    "        benefit_type_mapping\n",
    "    )\n",
    "\n",
    "    expanded_rows = []\n",
    "    for _, row in benefit_type.iterrows():\n",
    "        occupation_codes = occupation_mapping[row[\"Occupation Type\"]]\n",
    "        for code in occupation_codes:\n",
    "            new_row = row.copy()\n",
    "            new_row[\"Occupation\"] = code\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    benefit_type_transformed = pd.DataFrame(expanded_rows)\n",
    "    benefit_type_transformed = benefit_type_transformed.drop(\n",
    "        columns=[\"Occupation Type\"]\n",
    "    )\n",
    "    transformed[\"Incidence_Benefit_Type\"] = (\n",
    "        benefit_type_transformed.pivot_table(\n",
    "            index=[\"Sex\", \"Occupation\", \"Benefit Type\"],\n",
    "            columns=\"Type\",\n",
    "            values=\"Benefit Type Factor\",\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"Accident\": \"Accident Benefit Type Factor\",\n",
    "                \"Sickness\": \"Sick Benefit Type Factor\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 10. Incidence Duration Loading\n",
    "    df_duration_loading = assumptions_dict[\"Incidence_duration_loading\"].copy()\n",
    "    transformed[\"Incidence_Duration_Loading\"] = df_duration_loading.assign(\n",
    "        **{\n",
    "            \"Policy Duration (Curtate Years)\": lambda x: x[\n",
    "                \"Policy Duration (Curtate Years)\"\n",
    "            ].astype(str)\n",
    "        }\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"Accident\": \"Accident Duration Factor\",\n",
    "            \"Sickness\": \"Sick Duration Factor\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 11. Incidence Age Rates Sickness Combined\n",
    "    df_sickness_female = assumptions_dict[\"Incidence_age_rates_females\"][\n",
    "        [\"Sex\", \"Age\", \"Sickness\"]\n",
    "    ].copy()\n",
    "    df_sickness_male = assumptions_dict[\"Incidence_age_rates_males\"][\n",
    "        [\"Sex\", \"Age\", \"Sickness\"]\n",
    "    ].copy()\n",
    "\n",
    "    df_sickness_female = df_sickness_female.rename(\n",
    "        columns={\"Age\": \"Age LB\", \"Sex\": \"sex\", \"Sickness\": \"Sick Age Rates\"}\n",
    "    )\n",
    "    df_sickness_male = df_sickness_male.rename(\n",
    "        columns={\"Age\": \"Age LB\", \"Sex\": \"sex\", \"Sickness\": \"Sick Age Rates\"}\n",
    "    )\n",
    "\n",
    "    transformed[\"Incidence_Age_Rates_Sickness_Combined\"] = pd.concat(\n",
    "        [df_sickness_male, df_sickness_female], ignore_index=True\n",
    "    )\n",
    "\n",
    "    # 12. Death Only Mortality Floor\n",
    "    df_mortality_floor = assumptions_dict[\"DeathOnly_mort_floor\"].copy()\n",
    "    mortality_floor = df_mortality_floor[[\"Age LB\", \"225% MS\", \"225% FS\"]].rename(\n",
    "        columns={\"225% MS\": \"M\", \"225% FS\": \"F\"}\n",
    "    )\n",
    "    transformed[\"Death_Only_Mortality_Floor\"] = pd.melt(\n",
    "        mortality_floor,\n",
    "        id_vars=[\"Age LB\"],\n",
    "        var_name=\"sex\",\n",
    "        value_name=\"Mortality Floor\",\n",
    "    )\n",
    "\n",
    "    # 13. Termination Age Rates\n",
    "    df_termination_rates = assumptions_dict[\"Termination_age_rates\"].copy()\n",
    "    termination_rates = df_termination_rates.rename(\n",
    "        columns={\n",
    "            \"Age last birthday at last policy anniversary at Date of Disablement \\ Gender\": \"Age LB\",\n",
    "            \"Male\": \"M\",\n",
    "            \"Female\": \"F\",\n",
    "        }\n",
    "    )\n",
    "    transformed[\"Termination_Age_Rates\"] = pd.melt(\n",
    "        termination_rates,\n",
    "        id_vars=[\"Age LB\"],\n",
    "        var_name=\"sex\",\n",
    "        value_name=\"Termination Age Rates\",\n",
    "    )\n",
    "\n",
    "    # 14. Termination Smoker\n",
    "    df_termination_smoker = assumptions_dict[\"Termination_smoker\"].copy()\n",
    "    transformed[\"Termination_Smoker\"] = df_termination_smoker.rename(\n",
    "        columns={\"Smoker Status\": \"Smoker status\", \"Rate\": \"Termination Smoker status\"}\n",
    "    )\n",
    "\n",
    "    # 15. Termination Benefit Type\n",
    "    df_termination_benefit = assumptions_dict[\"Termination_benefit_type\"].copy()\n",
    "    benefit_type_mapping = {\"Agreed Value\": \"A\", \"Indemnity\": \"I\"}\n",
    "    transformed[\"Termination_Benefit_Type\"] = df_termination_benefit.rename(\n",
    "        columns={\"Rates\": \"Termination Benefit Type\"}\n",
    "    ).assign(**{\"Benefit Type\": lambda x: x[\"Benefit Type\"].map(benefit_type_mapping)})\n",
    "\n",
    "    # 16. Termination Duration Factor Accident\n",
    "    df_termination_duration_acc = assumptions_dict[\n",
    "        \"Termination_duration_factor_acc\"\n",
    "    ].copy()\n",
    "    transformed[\"Termination_Duration_Factor_Accident\"] = (\n",
    "        df_termination_duration_acc.rename(\n",
    "            columns={\n",
    "                \"Curtate Policy Year\": \"Policy Year_10+\",\n",
    "                \"Sex\": \"sex\",\n",
    "                \"Rates\": \"Accident Policy Duration Factor\",\n",
    "            }\n",
    "        )\n",
    "        .drop(columns=[\"Type\"])\n",
    "        .assign(**{\"Policy Year_10+\": lambda x: x[\"Policy Year_10+\"].astype(str)})\n",
    "    )\n",
    "\n",
    "    # 17. Termination Duration Claim Accident\n",
    "    df_termination_claim_acc = assumptions_dict[\"Termination_duration_claim_acc\"].copy()\n",
    "    transformed[\"Termination_Duration_Claim_Acc\"] = df_termination_claim_acc.rename(\n",
    "        columns={\n",
    "            \"Sex\": \"sex\",\n",
    "            \"Waiting_period\": \"Waiting Period\",\n",
    "            \"Rates\": \"Claim Waiting Occupation Factor\",\n",
    "        }\n",
    "    ).assign(**{\"Claim Duration\": lambda x: x[\"Claim Duration\"].astype(int)})\n",
    "\n",
    "    # 18. Termination Benefit Period\n",
    "    df_termination_benefit_period = assumptions_dict[\n",
    "        \"Termination_benefit_period\"\n",
    "    ].copy()\n",
    "    transformed[\"Termination_Benefit_Period\"] = df_termination_benefit_period.rename(\n",
    "        columns={\n",
    "            \"Duration since Disablement (Years***)\": \"Claim Duration_6+\",\n",
    "            \"Benefit Period\": \"Benefit Period_65+\",\n",
    "            \"Rates\": \"Benefit Period Factor\",\n",
    "        }\n",
    "    ).assign(\n",
    "        **{\n",
    "            \"Claim Duration_6+\": lambda x: x[\"Claim Duration_6+\"].astype(str),\n",
    "            \"Benefit Period_65+\": lambda x: x[\"Benefit Period_65+\"].astype(str),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 19. Termination Duration Factor Sickness\n",
    "    df_termination_duration_sick = assumptions_dict[\n",
    "        \"Termination_duration_factor_sic\"\n",
    "    ].copy()\n",
    "    transformed[\"Termination_Duration_Factor_Sickness\"] = (\n",
    "        df_termination_duration_sick.rename(\n",
    "            columns={\n",
    "                \"Curtate Policy Year\": \"Policy Year_10+\",\n",
    "                \"Sex\": \"sex\",\n",
    "                \"Rates\": \"Sickness Policy Duration Factor\",\n",
    "            }\n",
    "        )\n",
    "        .drop(columns=[\"Type\"])\n",
    "        .assign(**{\"Policy Year_10+\": lambda x: x[\"Policy Year_10+\"].astype(str)})\n",
    "    )\n",
    "\n",
    "    # 20. Termination Duration Claim Sickness\n",
    "    df_termination_claim_sick = assumptions_dict[\n",
    "        \"Termination_duration_claim_sick\"\n",
    "    ].copy()\n",
    "    transformed[\"Termination_Duration_Claim_Sick\"] = df_termination_claim_sick.rename(\n",
    "        columns={\n",
    "            \"Sex\": \"sex\",\n",
    "            \"Waiting_period\": \"Waiting Period\",\n",
    "            \"Rates\": \"Claim Waiting Occupation Factor\",\n",
    "        }\n",
    "    ).assign(**{\"Claim Duration\": lambda x: x[\"Claim Duration\"].astype(int)})\n",
    "\n",
    "    # 21. Inflation\n",
    "    df_inflation = assumptions_dict[\"Inflation\"].copy()\n",
    "    # 拆分年月日并重新组装\n",
    "    df_inflation[\"Year_Year\"] = df_inflation[\"Year\"].dt.year\n",
    "    df_inflation[\"Year_Month\"] = df_inflation[\"Year\"].dt.day\n",
    "    df_inflation[\"Year_Day\"] = df_inflation[\"Year\"].dt.month\n",
    "    df_inflation[\"Date\"] = pd.to_datetime(\n",
    "        df_inflation[\"Year_Year\"].astype(str)\n",
    "        + \"-\"\n",
    "        + df_inflation[\"Year_Month\"].astype(str)\n",
    "        + \"-\"\n",
    "        + df_inflation[\"Year_Day\"].astype(str)\n",
    "    )\n",
    "    # 删除临时列\n",
    "    df_inflation.drop(\n",
    "        columns=[\"Year\", \"Year_Year\", \"Year_Month\", \"Year_Day\"], inplace=True\n",
    "    )\n",
    "    # columnd排序，先Date，在CPI\n",
    "    df_inflation = df_inflation[[\"Date\", \"CPI\"]]\n",
    "    transformed[\"Inflation\"] = df_inflation\n",
    "\n",
    "    # 21. Forward Rate\n",
    "    df_forward = assumptions_dict[\"Forward_rates\"].copy()\n",
    "    # 对 Forward Rate 做同样的处理\n",
    "    df_forward[\"Year_Year\"] = df_forward[\"Month\"].dt.year\n",
    "    df_forward[\"Year_Month\"] = df_forward[\"Month\"].dt.day\n",
    "    df_forward[\"Year_Day\"] = df_forward[\"Month\"].dt.month\n",
    "    df_forward[\"Month\"] = pd.to_datetime(\n",
    "        df_forward[\"Year_Year\"].astype(str)\n",
    "        + \"-\"\n",
    "        + df_forward[\"Year_Month\"].astype(str)\n",
    "        + \"-\"\n",
    "        + df_forward[\"Year_Day\"].astype(str)\n",
    "    )\n",
    "    # 删除临时列\n",
    "    df_forward.drop(columns=[\"Year_Year\", \"Year_Month\", \"Year_Day\"], inplace=True)\n",
    "    transformed[\"Forward_rate\"] = df_forward\n",
    "\n",
    "    return transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions = transform_assumptions(assumptions_dict)\n",
    "print(assumptions[\"Forward_rate\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelx as mx\n",
    "\n",
    "model = mx.read_model(\"/Users/user/Downloads/Basic_IP_Model_v11_withDateTimeFix&SmokerStatusFix\")\n",
    "\n",
    "model.Assumptions.Forward_rate  \n",
    "\n",
    "model.Results.cashflow_output_t0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformed assumptions DataFrame\n",
    "print(\"Transformed Assumptions DataFrame:\")\n",
    "print(assumptions[\"Forward_rate\"].dtypes)\n",
    "print(\"\\nSample row type:\", type(assumptions[\"Forward_rate\"][\"Year\"].iloc[0]))\n",
    "print(\"\\n-------------------\\n\")\n",
    "\n",
    "# For modelx Assumptions DataFrame\n",
    "print(\"ModelX Assumptions DataFrame:\")\n",
    "print(model.Assumptions.Forward_rate.dtypes)\n",
    "print(\"\\nSample row type:\", type(model.Assumptions.Forward_rate[\"Year\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Results.cashflow_output_t0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.Results.cashflow_output_t0()\n",
    "# Check duplicates based on Policy Number\n",
    "print(\"Duplicates by Policy Number:\")\n",
    "print(df['Policy Number'].duplicated().any())\n",
    "print(\"\\nDuplicate Policy Numbers if any:\")\n",
    "print(df[df['Policy Number'].duplicated(keep=False)].sort_values('Policy Number'))\n",
    "\n",
    "# Check duplicates based on multiple columns\n",
    "print(\"\\nDuplicates by Policy Number, t, Related_Policy_Group:\")\n",
    "print(df.duplicated(['Policy Number', 't', 'Related_Policy_Group']).any())\n",
    "\n",
    "# Get a count of each Policy Number\n",
    "print(\"\\nPolicy Number value counts:\")\n",
    "print(df['Policy Number'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_assumptions_to_excel(transformed_dict, output_path):\n",
    "    \"\"\"\n",
    "    Export transformed assumptions dictionary to Excel file with multiple sheets\n",
    "    \n",
    "    Args:\n",
    "        transformed_dict: Dictionary of transformed DataFrames\n",
    "        output_path: Path where the Excel file should be saved (including filename)\n",
    "    \n",
    "    Example:\n",
    "        export_assumptions_to_excel(transformed_data, \"transformed_assumptions.xlsx\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create Excel writer object\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            # Iterate through the dictionary and write each DataFrame to a sheet\n",
    "            for sheet_name, df in transformed_dict.items():\n",
    "                # Ensure the sheet name is valid (Excel has a 31 character limit)\n",
    "                valid_sheet_name = sheet_name[:31]\n",
    "                \n",
    "                # Write the DataFrame to Excel\n",
    "                df.to_excel(writer, sheet_name=valid_sheet_name, index=False)\n",
    "                \n",
    "                # Auto-adjust columns width\n",
    "                worksheet = writer.sheets[valid_sheet_name]\n",
    "                for idx, col in enumerate(df.columns):\n",
    "                    max_length = max(\n",
    "                        df[col].astype(str).apply(len).max(),  # max length of values\n",
    "                        len(str(col))  # length of column name\n",
    "                    )\n",
    "                    worksheet.column_dimensions[chr(65 + idx)].width = max_length + 2  # Add padding\n",
    "                \n",
    "        print(f\"Successfully exported to {output_path}\")\n",
    "        print(f\"Number of sheets exported: {len(transformed_dict)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting to Excel: {str(e)}\")\n",
    "\n",
    "\n",
    "export_assumptions_to_excel(\n",
    "    assumptions, \n",
    "    \"transformed_assumptions.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mpf_validation import validate_mpf_dataframe\n",
    "\n",
    "# 准备数据\n",
    "df_mpf = pd.read_excel(\"MPF_Data_Validation_Check_Sample.xlsx\", sheet_name=\"MPF_Input\"   )  # 你的MPF数据\n",
    "df_rules = pd.read_excel(\"MPF_Data_Validation_Check_Sample.xlsx\", sheet_name=\"Rules_Input\")  # 你的规则数据\n",
    "\n",
    "# 运行验证\n",
    "results, cleaned_df, invalid_rows = validate_mpf_dataframe(df_mpf, df_rules , \"2025-03-11\")\n",
    "\n",
    "# 处理结果\n",
    "if not invalid_rows.empty:\n",
    "    print(f\"Found {len(invalid_rows)} invalid rows\")\n",
    "    # 决定是否使用清理后的数据\n",
    "    # ...\n",
    "else:\n",
    "    print(\"No invalid rows found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1, 2, 3]\n",
    "a = b\n",
    "b.append(4)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
